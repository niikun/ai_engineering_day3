{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNguiZcFPa2oVmDPbpTbep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niikun/ai_engineering_day3/blob/main/DualRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DualRAG RAG + Deep検索\n",
        "https://arxiv.org/abs/2504.18243   \n",
        "https://zenn.dev/knowledgesense/articles/10b2b5f772b810"
      ],
      "metadata": {
        "id": "lp0dYvoHx4Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAGのソース文書に、正解が「そのまま」あればそれでOKですが、現実では普通、情報が分散しています。  \n",
        "通常のRAGだと、ユーザーの質問に対して1回しか検索しないので、回答精度が悪いです。  \n",
        "【ユーザーが質問を入力して来たとき】  \n",
        "\n",
        "1.推論・検索（RaQ）\n",
        "- 「現在の知識で十分か？」をメタ認知→「情報不足」と判断すると検索  \n",
        "- 的確なキーワードで検索できるよう、無駄な単語は省く（=エンティティの抽出）\n",
        "2.情報の要約（pKA）\n",
        "- 1で取得した情報を整理・要約\n",
        "3.1・2の繰り返し\n",
        "- 回答にたどり着くまで、必要なだけこのサイクルを繰り返す\n",
        "4.最終回答を生成  \n",
        "\n",
        "DualRAGという手法のキモは、「検索⇄要約」のサイクルです。単に検索回数を多くするのではなく、常に「メタ的な視点から要約・整理するステップ」を挟みます。そうすることで、最低限の検索回数で、答えにたどり着くことが可能です。"
      ],
      "metadata": {
        "id": "J8Ryk6ic5arg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith langchain_openai langchain-community langchain-chroma"
      ],
      "metadata": {
        "id": "45qSzeXqxkiw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "gold_answer = \"「Inference Time Scaling」とは、推論時に計算量を増やしてモデルの性能を高める手法です。これはモデルのサイズを大きくする代わりに、難しい入力に対して多くの計算リソースを使うことで、より良い出力を得ようとするアプローチです。\""
      ],
      "metadata": {
        "id": "-4OqUrPkzOta"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 演習用のコンテンツを取得\n",
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew-ETw7txoW2",
        "outputId": "02c2b7cd-2482-46c3-8392-bd8179e7c5e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lecture-ai-engineering'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Total 52 (delta 0), reused 0 (delta 0), pack-reused 52 (from 1)\u001b[K\n",
            "Receiving objects: 100% (52/52), 83.21 KiB | 5.55 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('lang_smith')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "%env LANGCHAIN_TRACING_V2=true\n",
        "%env LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "%env LANGSMITH_PROJECT=\"DualRAG\"\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
        "\n",
        "# テキストローダーの準備\n",
        "loader = TextLoader(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "# テキストのチャンク分け\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,chunk_overlap=200\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# チャンクの埋め込み準備\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "db = Chroma.from_documents(all_splits,embeddings)\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF_OW-ONxt2M",
        "outputId": "6dee8952-dbf7-44df-a17c-a761fbd1562c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1189, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1284, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1396, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1373, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1689, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1077, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1350, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1312, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LANGCHAIN_TRACING_V2=true\n",
            "env: LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
            "env: LANGSMITH_PROJECT=\"DualRAG\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXz8bB-mxcXe",
        "outputId": "36ccea9b-e316-42be-9a20-4009404aec50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM（大規模言語モデル）におけるInference Time Scaling（推論時スケーリング）は、モデルが学習時にパラメータ数やデータ量を増やすのではなく、推論時に使用する計算資源を増加させるアプローチを指します。この概念は、推論時の計算能力を高めることで、モデルの応答速度や精度を向上させることを目指しています。\n",
            "\n",
            "具体的には、推論時の計算資源を増やすことによって、以下のようなメリットが得られる可能性があります：\n",
            "\n",
            "1. **応答の精度向上**: 簡単な質問には迅速に答えを得ることができ、複雑な問題に対してはより多くの計算資源を投入することで、深い考察やより高精度の回答が可能になる。\n",
            "\n",
            "2. **柔軟な応答生成**: モデルが状況に応じて計算資源を動的に調整できることで、条件に応じた最適な応答が実現できる。\n",
            "\n",
            "3. **リソースの効率化**: 特定のタスクや文脈に合わせて計算資源を調整することによって、高いパフォーマンスを維持しながらコストを管理することができる。\n",
            "\n",
            "最近の研究、特にGoogle DeepMindの発表などは、同じ計算資源条件下でパラメータを増やすよりも、推論時の資源を増やすことがより効果的である可能性があることを示唆しています。ただし、この選択が最良であるかどうかはタスク依存であるため、使用する状況や目的に応じた適切な判断が求められます。\n",
            "\n",
            "全体として、Inference Time Scalingは、LLMの性能を向上させるための新しいアプローチとして注目されており、モデルの能力を最大限に引き出す鍵となる要素とされています。\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict, Any\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "\n",
        "retriever: BaseRetriever = retriever\n",
        "\n",
        "# プロンプト定義\n",
        "meta_prompt = ChatPromptTemplate.from_template(\n",
        "    \"ユーザーの質問: {question}\\n\"\n",
        "    \"現時点の文脈:\\n{context}\\n\"\n",
        "    \"→ 上記の情報だけで回答できますか？\"\n",
        ")\n",
        "\n",
        "extract_prompt = ChatPromptTemplate.from_template(\n",
        "    \"質問: {question}\\n\"\n",
        "    \"→ 検索に使うキーワード（エンティティ）を列挙してください。\"\n",
        ")\n",
        "\n",
        "summarize_prompt = ChatPromptTemplate.from_template(\n",
        "    \"検索結果:\\n{snippets}\\n\"\n",
        "    \"→ 上記を要点だけ短く要約してください。\"\n",
        ")\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_template(\n",
        "    \"最終コンテキスト:\\n{context}\\n\"\n",
        "    \"質問: {question}\\n\"\n",
        "    \"→ 回答を生成してください。\"\n",
        ")\n",
        "\n",
        "def iterative_rag(question: str, max_cycles: int = 3) -> str:\n",
        "    context = \"\"  # 初期文脈は空、もしくは事前知識\n",
        "    for cycle in range(max_cycles):\n",
        "        # 1) メタ認知フェーズ（RaQ）\n",
        "        msg = meta_prompt.format_messages(context=context, question=question)\n",
        "        decision = llm(msg).content\n",
        "        if \"回答可能\" in decision:\n",
        "            break\n",
        "\n",
        "        # 2) キーワード抽出\n",
        "        msg = extract_prompt.format_messages(question=question)\n",
        "        keywords = llm(msg).content\n",
        "\n",
        "        # 3) 検索フェーズ\n",
        "        docs: List[Document] = retriever.get_relevant_documents(keywords)\n",
        "        snippets = \"\\n\".join(d.page_content for d in docs[:5])\n",
        "\n",
        "        # 4) 要約フェーズ（pKA）\n",
        "        msg = summarize_prompt.format_messages(snippets=snippets)\n",
        "        summary = llm(msg).content\n",
        "\n",
        "        # 5) コンテキストに追加\n",
        "        context += \"\\n\" + summary\n",
        "\n",
        "    # 6) 最終回答フェーズ\n",
        "    msg = final_prompt.format_messages(context=context, question=question)\n",
        "    return llm(msg).content\n",
        "\n",
        "# 使い方\n",
        "answer = iterative_rag(question)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "OPEN_AI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=OPEN_AI_API_KEY, max_retries=5, timeout=60)\n",
        "\n",
        "def openai_generator(query):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\":query\n",
        "        }\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def evaluate_answer_accuracy(query, response, reference):\n",
        "\n",
        "    template_accuracy1 = (\n",
        "          \"Instruction: You are a world class state of the art assistant for rating \"\n",
        "          \"a User Answer given a Question. The Question is completely answered by the Reference Answer.\\n\"\n",
        "          \"Say 4, if User Answer is full contained and equivalent to Reference Answer\"\n",
        "          \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n",
        "          \"Say 2, if User Answer is partially contained and almost equivalent to Reference Answer\"\n",
        "          \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n",
        "          \"Say 0, if User Answer is not contained in Reference Answer or not accurate in all terms, topics,\"\n",
        "          \"numbers, metrics, dates and units or the User Answer do not answer the question.\\n\"\n",
        "          \"Do not explain or justify your rating. Your rating must be only 4, 2 or 0 according to the instructions above.\\n\"\n",
        "          \"Even small discrepancies in meaning, terminology, directionality, or implication must result in a lower score. Only rate 4 if the User Answer is a complete and precise match to the Reference Answer in every aspect.\\n\"\n",
        "          \"### Question: {query}\\n\"\n",
        "          \"### {answer0}: {sentence_inference}\\n\"\n",
        "          \"### {answer1}: {sentence_true}\\n\"\n",
        "          \"The rating is:\\n\"\n",
        "      )\n",
        "    template_accuracy2 = (\n",
        "          \"I will rate the User Answer in comparison to the Reference Answer for a given Question.\\n\"\n",
        "          \"A rating of 4 indicates that the User Answer is entirely consistent with the Reference Answer, covering all aspects, topics, numbers, metrics, dates, and units.\\n\"\n",
        "          \"A rating of 2 signifies that the User Answer is mostly aligned with the Reference Answer, with minor discrepancies in some areas.\\n\"\n",
        "          \"A rating of 0 means that the User Answer is either inaccurate, incomplete, or unrelated to the Reference Answer, or it fails to address the Question.\\n\"\n",
        "          \"I will provide the rating without any explanation or justification, adhering to the following scale: 0 (no match), 2 (partial match), 4 (exact match).\\n\"\n",
        "          \"Even minor inconsistencies in meaning, terminology, emphasis, or factual detail should prevent a rating of 4. Only assign a 4 if the User Answer exactly and unambiguously matches the Reference Answer in every respect.\"\n",
        "          \"Do not explain or justify my rating. My rating must be only 4, 2 or 0 only.\\n\\n\"\n",
        "          \"Question: {query}\\n\\n\"\n",
        "          \"{answer0}: {sentence_inference}\\n\\n\"\n",
        "          \"{answer1}: {sentence_true}\\n\\n\"\n",
        "          \"Rating: \"\n",
        "      )\n",
        "\n",
        "    score1 = openai_generator(\n",
        "                template_accuracy1.format(\n",
        "                      query=query,\n",
        "                      answer0=\"User Answer\",\n",
        "                      answer1=\"Reference Answer\",\n",
        "                      sentence_inference=response,\n",
        "                      sentence_true=reference,\n",
        "                    )\n",
        "                )\n",
        "    try:\n",
        "      score1 = int(score1)\n",
        "    except:\n",
        "      print(\"Failed\")\n",
        "      score1 = 0\n",
        "\n",
        "    score2 = openai_generator(\n",
        "                template_accuracy2.format(\n",
        "                        query=query,\n",
        "                        answer0=\"Reference Answer\",\n",
        "                        answer1=\"User Answer\",\n",
        "                        sentence_inference=reference,\n",
        "                        sentence_true=response,\n",
        "                    )\n",
        "                  )\n",
        "\n",
        "    try:\n",
        "      score2 = int(score2)\n",
        "    except:\n",
        "      print(\"Failed\")\n",
        "      score2 = 0\n",
        "\n",
        "\n",
        "    return (score1 + score2) / 2"
      ],
      "metadata": {
        "id": "pXl3N5gIzkWK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_answer_accuracy(question, answer, gold_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHW7ksaQ0rP1",
        "outputId": "7e89ff0f-7b24-4492-a894-96c50229dfe7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3X1P4Gp0sEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}